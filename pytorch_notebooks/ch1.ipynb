{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: 2.0\n",
      "Loss: 0.5\n",
      "Gradient with respect to x: -2.0\n",
      "Gradient with respect to weight: -1.0\n",
      "Gradient with respect to bias: -1.0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "This script demonstrates a simple dynamic computation graph in PyTorch using a\n",
    "linear neuron (without activation). We use a realistic loss (mean squared error)\n",
    "and manually set parameters and inputs to simple, predictable values.\n",
    "The expected behavior is as follows:\n",
    "\n",
    "Given:\n",
    "    - Neuron function: f(x) = weight * x + bias\n",
    "    - Set weight = 2.0 and bias = 0.0 (manually assigned)\n",
    "    - Input x = 1.0, so the neuron output will be f(1.0) = 2.0\n",
    "    - Define a target value of 3.0\n",
    "\n",
    "Loss:\n",
    "    - We use mean squared error (MSE) loss:\n",
    "      loss = 0.5 * (f(x) - target)^2\n",
    "    - With f(1.0) = 2.0 and target = 3.0, we get:\n",
    "      loss = 0.5 * (2.0 - 3.0)^2 = 0.5 * 1^2 = 0.5\n",
    "\n",
    "Gradients:\n",
    "    - The gradient of the loss with respect to the output is (f(x) - target) = -1.0.\n",
    "    - For a linear function, the gradients are:\n",
    "        * d(loss)/d(weight) = (f(x) - target) * x = -1.0 * 1.0 = -1.0\n",
    "        * d(loss)/d(bias)   = (f(x) - target) = -1.0\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple linear neuron (without an activation function).\n",
    "class SimpleLinearNeuron(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the SimpleLinearNeuron model.\n",
    "        Instead of random initialization, we manually set the weight and bias\n",
    "        to known values for predictability in our example.\n",
    "        \"\"\"\n",
    "        super(SimpleLinearNeuron, self).__init__()\n",
    "        # Manually create parameters with known values.\n",
    "        # We wrap the initial values in torch.tensor and set requires_grad=True.\n",
    "        self.weight = nn.Parameter(torch.tensor([2.0]))\n",
    "        self.bias = nn.Parameter(torch.tensor([0.0]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the neuron.\n",
    "        The computation is a simple linear transformation:\n",
    "            f(x) = weight * x + bias\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "        Returns:\n",
    "            torch.Tensor: The computed output.\n",
    "        \"\"\"\n",
    "        return self.weight * x + self.bias\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function that demonstrates:\n",
    "    1. Creating a neuron with predetermined parameters.\n",
    "    2. Computing the output for a given input.\n",
    "    3. Calculating a realistic mean squared error loss.\n",
    "    4. Performing backpropagation to compute gradients.\n",
    "    5. Printing the output, loss, and gradients.\n",
    "    \"\"\"\n",
    "    # Instantiate the neuron with fixed weight and bias.\n",
    "    neuron = SimpleLinearNeuron()\n",
    "    \n",
    "    # Create an input tensor with the value 1.0.\n",
    "    # We set requires_grad=True so that PyTorch tracks operations on x.\n",
    "    x = torch.tensor([1.0], requires_grad=True)\n",
    "    \n",
    "    # Define the target value for our neuron output.\n",
    "    target = torch.tensor([3.0])\n",
    "    \n",
    "    # Forward pass: compute the neuron's output.\n",
    "    # With weight=2.0 and bias=0.0, the expected output is 2.0.\n",
    "    output = neuron(x)\n",
    "    \n",
    "    # Compute the mean squared error loss.\n",
    "    # We use 0.5 * (output - target)^2 so that the derivative is simplified.\n",
    "    loss = 0.5 * (output - target) ** 2\n",
    "    \n",
    "    # Perform backpropagation to compute gradients.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Expected values:\n",
    "    # Output: 2.0\n",
    "    # Loss: 0.5\n",
    "    # d(loss)/d(output) = (2.0 - 3.0) = -1.0\n",
    "    # Gradient with respect to weight: -1.0 * 1.0 = -1.0\n",
    "    # Gradient with respect to bias: -1.0\n",
    "    \n",
    "    # Print the computed output.\n",
    "    print(\"Output:\", output.item())\n",
    "    \n",
    "    # Print the computed loss.\n",
    "    print(\"Loss:\", loss.item())\n",
    "    \n",
    "    # Print the gradient with respect to the input x.\n",
    "    print(\"Gradient with respect to x:\", x.grad.item())\n",
    "    \n",
    "    # Print the gradient with respect to the neuron's weight.\n",
    "    print(\"Gradient with respect to weight:\", neuron.weight.grad.item())\n",
    "    \n",
    "    # Print the gradient with respect to the neuron's bias.\n",
    "    print(\"Gradient with respect to bias:\", neuron.bias.grad.item())\n",
    "\n",
    "# Standard boilerplate to run the main function.\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
